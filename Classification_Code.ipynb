{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBOeCtXYuJVN",
        "outputId": "ad68cd56-d8c1-4834-9e16-5a06025e7702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Class Distribution:\n",
            "Class 0 (Real) : 2142 images\n",
            "Class 1 (AI)   : 2117 images\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "def count_images_recursive(folder):\n",
        "    count = 0\n",
        "    for root, _, files in os.walk(folder):\n",
        "        count += len([\n",
        "            f for f in files \n",
        "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "        ])\n",
        "    return count\n",
        "\n",
        "# LOCAL paths\n",
        "# Assuming folders are in same directory as notebook:\n",
        "root_dir = os.getcwd()   # current working dir (the folder where your .ipynb is)\n",
        "real_path = os.path.join(root_dir, \"Real_5k\")\n",
        "ai_path = os.path.join(root_dir, \"AI_5k\")\n",
        "\n",
        "# Count images\n",
        "real_count = count_images_recursive(real_path)\n",
        "ai_count = count_images_recursive(ai_path)\n",
        "\n",
        "# Print results\n",
        "print(\"✅ Class Distribution:\")\n",
        "print(f\"Class 0 (Real) : {real_count} images\")\n",
        "print(f\"Class 1 (AI)   : {ai_count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3npGvYlvuWes",
        "outputId": "0656b702-eb67-4606-dd4c-d9949eaaadf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Total Samples: 4259\n",
            " - Real [0]: 2142\n",
            " - AI   [1]: 2117\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Collect image paths from both classes\n",
        "real_images = [\n",
        "    os.path.join(root, f)\n",
        "    for root, _, files in os.walk(real_path)\n",
        "    for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "]\n",
        "\n",
        "ai_images = [\n",
        "    os.path.join(root, f)\n",
        "    for root, _, files in os.walk(ai_path)\n",
        "    for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "]\n",
        "\n",
        "# Combine image paths and labels\n",
        "all_samples = real_images + ai_images\n",
        "all_labels  = [0] * len(real_images) + [1] * len(ai_images)\n",
        "\n",
        "# Summary\n",
        "print(f\" Total Samples: {len(all_samples)}\")\n",
        "print(f\" - Real [0]: {len(real_images)}\")\n",
        "print(f\" - AI   [1]: {len(ai_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SzErs3SudoF",
        "outputId": "e1193baa-176e-4f9c-b77b-e2869df14f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset split:\n",
            " - Training    : 2982 samples\n",
            " - Validation  : 851 samples\n",
            " - Test        : 426 samples\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# => X_test will be exactly 10% of the total dataset\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    all_samples, all_labels,\n",
        "    test_size=0.1,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#Split the remaining 90% into:\n",
        "# => 70% Train  (0.9 × 0.778 ≈ 70%)\n",
        "# => 20% Val    (0.9 × 0.222 ≈ 20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval,\n",
        "    test_size=0.222,  # 0.222 × 90% ≈ 20% of total\n",
        "    stratify=y_trainval,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\" Dataset split:\")\n",
        "print(f\" - Training    : {len(X_train)} samples\")   # ~70%\n",
        "print(f\" - Validation  : {len(X_val)} samples\")     # ~20%\n",
        "print(f\" - Test        : {len(X_test)} samples\")     # 10%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVHqBMVpvCYS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, samples, labels, transform=None):\n",
        "        self.samples = samples\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.samples[idx]\n",
        "        label = self.labels[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # If image can't be loaded, use a black fallback\n",
        "        if img is None:\n",
        "            img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "        else:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert for PIL use\n",
        "\n",
        "        # Resize to 224x224\n",
        "        img = cv2.resize(img, (224, 224))\n",
        "        pil_img = Image.fromarray(img)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            pil_img = self.transform(pil_img)\n",
        "\n",
        "        return pil_img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tdEwL5RawPJs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_ds = FaceDataset(X_train, y_train, transform=train_tf)\n",
        "val_ds   = FaceDataset(X_val, y_val, transform=test_tf)\n",
        "test_ds  = FaceDataset(X_test, y_test, transform=test_tf)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32_38X_ZwQor",
        "outputId": "9d9a044c-7c03-4c2c-b235-7011926edee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"✅ Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNOy8-xkwbGz"
      },
      "outputs": [],
      "source": [
        "# ✅ Dropout-based classification head for ViT\n",
        "class MCDropoutHead(nn.Module):\n",
        "    def __init__(self, in_features, num_classes=2, dropout_prob=0.4):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ✅ ResNet50 with standard Linear head\n",
        "def get_resnet():\n",
        "    model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    return model.to(device)\n",
        "\n",
        "# ✅ ViT with MCDropout head\n",
        "def get_vit():\n",
        "    model = models.vit_b_16(weights='IMAGENET1K_V1')\n",
        "    in_features = model.heads[0].in_features\n",
        "    model.heads = nn.Sequential(MCDropoutHead(in_features, num_classes=2, dropout_prob=0.4))\n",
        "    return model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhXj_PxtyOtu",
        "outputId": "2a964c81-ae4c-4774-c0d9-0aa059adc1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\abura/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:05<00:00, 17.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\abura/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 330M/330M [00:16<00:00, 20.9MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Load models\n",
        "resnet = get_resnet()\n",
        "vit = get_vit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DV1bBeoUyRVc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/30] Train Loss: 0.0907 | Train Acc: 96.51% || Val Loss: 0.1792 | Val Acc: 93.42%\n",
            "[Epoch 2/30] Train Loss: 0.0591 | Train Acc: 97.79% || Val Loss: 0.1789 | Val Acc: 94.01%\n",
            "[Epoch 3/30] Train Loss: 0.0526 | Train Acc: 98.09% || Val Loss: 0.1869 | Val Acc: 93.42%\n",
            "[Epoch 4/30] Train Loss: 0.0285 | Train Acc: 98.99% || Val Loss: 0.1755 | Val Acc: 94.48%\n",
            "[Epoch 5/30] Train Loss: 0.0467 | Train Acc: 98.39% || Val Loss: 0.1491 | Val Acc: 94.95%\n",
            "[Epoch 6/30] Train Loss: 0.0352 | Train Acc: 99.03% || Val Loss: 0.1947 | Val Acc: 93.07%\n",
            "[Epoch 7/30] Train Loss: 0.0496 | Train Acc: 98.16% || Val Loss: 0.1687 | Val Acc: 93.18%\n",
            "[Epoch 8/30] Train Loss: 0.0402 | Train Acc: 98.69% || Val Loss: 0.1870 | Val Acc: 93.07%\n",
            "[Epoch 9/30] Train Loss: 0.0603 | Train Acc: 97.85% || Val Loss: 0.1558 | Val Acc: 94.01%\n",
            "⛔ Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "# Train ResNet\n",
        "resnet = train_model(\n",
        "    resnet,\n",
        "    torch.optim.Adam(resnet.parameters(), lr=1e-4),\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=30,\n",
        "    patience=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dpied4nIyS3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/30] Train Loss: 0.4988 | Train Acc: 75.55% || Val Loss: 0.3790 | Val Acc: 82.26%\n",
            "[Epoch 2/30] Train Loss: 0.2381 | Train Acc: 89.27% || Val Loss: 0.2282 | Val Acc: 90.72%\n",
            "[Epoch 3/30] Train Loss: 0.1674 | Train Acc: 93.46% || Val Loss: 0.2928 | Val Acc: 88.84%\n",
            "[Epoch 4/30] Train Loss: 0.0935 | Train Acc: 96.71% || Val Loss: 0.2455 | Val Acc: 91.07%\n",
            "[Epoch 5/30] Train Loss: 0.1045 | Train Acc: 96.28% || Val Loss: 0.1628 | Val Acc: 94.12%\n",
            "[Epoch 6/30] Train Loss: 0.0469 | Train Acc: 98.16% || Val Loss: 0.2482 | Val Acc: 92.01%\n",
            "[Epoch 7/30] Train Loss: 0.0544 | Train Acc: 97.79% || Val Loss: 0.3271 | Val Acc: 90.25%\n",
            "[Epoch 8/30] Train Loss: 0.0578 | Train Acc: 97.52% || Val Loss: 0.3396 | Val Acc: 89.07%\n",
            "[Epoch 9/30] Train Loss: 0.0304 | Train Acc: 98.89% || Val Loss: 0.3252 | Val Acc: 90.83%\n",
            "⛔ Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "# Train ViT (with MC Dropout)\n",
        "vit = train_model(\n",
        "    vit,\n",
        "    torch.optim.Adam(vit.parameters(), lr=5e-5),\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=30,\n",
        "    patience=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etBMixRNzGP0"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"checkpoint_vit.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HqZ43M5AzFT5"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(vit\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_trained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# (Optional) Download if on Colab\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      7\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_trained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_trained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# Save models\n",
        "torch.save(resnet.state_dict(), \"resnet_trained.pth\")\n",
        "torch.save(vit.state_dict(), \"vit_trained.pth\")\n",
        "\n",
        "# (Optional) Download if on Colab\n",
        "from google.colab import files\n",
        "files.download(\"resnet_trained.pth\")\n",
        "files.download(\"vit_trained.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e7Qvh9yg0egZ"
      },
      "outputs": [],
      "source": [
        "def ensemble_predict(model1, model2, loader):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            out1 = F.softmax(model1(imgs), dim=1)\n",
        "            out2 = F.softmax(model2(imgs), dim=1)\n",
        "            final = (out1 + out2) / 2\n",
        "            preds = final.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dBIScOZT22Te"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 🔍 Evaluate a single model\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# 📊 Evaluate all splits for a model\n",
        "def evaluate_all_splits(model, name=\"Model\"):\n",
        "    for title, loader in zip([\"Train\", \"Validation\", \"Test\"], [train_loader, val_loader, test_loader]):\n",
        "        y_true, y_pred = evaluate_model(model, loader)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        print(f\"\\n📊 {name} - {title}\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_true, y_pred, target_names=[\"Real\", \"AI\"]))\n",
        "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# 🤝 Soft voting ensemble\n",
        "def evaluate_ensemble(model1, model2, loader):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            out1 = F.softmax(model1(imgs), dim=1)\n",
        "            out2 = F.softmax(model2(imgs), dim=1)\n",
        "            final = (out1 + out2) / 2\n",
        "            preds = final.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# 📊 Evaluate ensemble across all splits\n",
        "def evaluate_all_ensemble(model1, model2, name=\"Ensemble\"):\n",
        "    for title, loader in zip([\"Train\", \"Validation\", \"Test\"], [train_loader, val_loader, test_loader]):\n",
        "        y_true, y_pred = evaluate_ensemble(model1, model2, loader)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        print(f\"\\n🤝 {name} - {title}\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_true, y_pred, target_names=[\"Real\", \"AI\"]))\n",
        "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wEpl2lu24jG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 ResNet - Train\n",
            "Accuracy: 0.9899\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.98      1.00      0.99      1500\n",
            "          AI       1.00      0.98      0.99      1482\n",
            "\n",
            "    accuracy                           0.99      2982\n",
            "   macro avg       0.99      0.99      0.99      2982\n",
            "weighted avg       0.99      0.99      0.99      2982\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1496    4]\n",
            " [  26 1456]]\n",
            "\n",
            "📊 ResNet - Validation\n",
            "Accuracy: 0.9401\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.92      0.96      0.94       428\n",
            "          AI       0.96      0.92      0.94       423\n",
            "\n",
            "    accuracy                           0.94       851\n",
            "   macro avg       0.94      0.94      0.94       851\n",
            "weighted avg       0.94      0.94      0.94       851\n",
            "\n",
            "Confusion Matrix:\n",
            " [[412  16]\n",
            " [ 35 388]]\n",
            "\n",
            "📊 ResNet - Test\n",
            "Accuracy: 0.9249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.90      0.96      0.93       214\n",
            "          AI       0.96      0.89      0.92       212\n",
            "\n",
            "    accuracy                           0.92       426\n",
            "   macro avg       0.93      0.92      0.92       426\n",
            "weighted avg       0.93      0.92      0.92       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[206   8]\n",
            " [ 24 188]]\n"
          ]
        }
      ],
      "source": [
        "# ✅ Run evaluations\n",
        "evaluate_all_splits(resnet, \"ResNet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SHJo6xHg3Coc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 ViT - Train\n",
            "Accuracy: 0.9869\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      0.98      0.99      1500\n",
            "          AI       0.98      1.00      0.99      1482\n",
            "\n",
            "    accuracy                           0.99      2982\n",
            "   macro avg       0.99      0.99      0.99      2982\n",
            "weighted avg       0.99      0.99      0.99      2982\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1464   36]\n",
            " [   3 1479]]\n",
            "\n",
            "📊 ViT - Validation\n",
            "Accuracy: 0.9083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.85      0.90       428\n",
            "          AI       0.86      0.97      0.91       423\n",
            "\n",
            "    accuracy                           0.91       851\n",
            "   macro avg       0.91      0.91      0.91       851\n",
            "weighted avg       0.92      0.91      0.91       851\n",
            "\n",
            "Confusion Matrix:\n",
            " [[362  66]\n",
            " [ 12 411]]\n",
            "\n",
            "📊 ViT - Test\n",
            "Accuracy: 0.8944\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.96      0.83      0.89       214\n",
            "          AI       0.85      0.96      0.90       212\n",
            "\n",
            "    accuracy                           0.89       426\n",
            "   macro avg       0.90      0.89      0.89       426\n",
            "weighted avg       0.90      0.89      0.89       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[177  37]\n",
            " [  8 204]]\n"
          ]
        }
      ],
      "source": [
        "evaluate_all_splits(vit, \"ViT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0wVyJTX_3EM2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🤝 ResNet + ViT Ensemble - Train\n",
            "Accuracy: 1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       1.00      1.00      1.00      1500\n",
            "          AI       1.00      1.00      1.00      1482\n",
            "\n",
            "    accuracy                           1.00      2982\n",
            "   macro avg       1.00      1.00      1.00      2982\n",
            "weighted avg       1.00      1.00      1.00      2982\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1500    0]\n",
            " [   0 1482]]\n",
            "\n",
            "🤝 ResNet + ViT Ensemble - Validation\n",
            "Accuracy: 0.9612\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.95      0.96       428\n",
            "          AI       0.95      0.97      0.96       423\n",
            "\n",
            "    accuracy                           0.96       851\n",
            "   macro avg       0.96      0.96      0.96       851\n",
            "weighted avg       0.96      0.96      0.96       851\n",
            "\n",
            "Confusion Matrix:\n",
            " [[408  20]\n",
            " [ 13 410]]\n",
            "\n",
            "🤝 ResNet + ViT Ensemble - Test\n",
            "Accuracy: 0.9554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.94      0.96       214\n",
            "          AI       0.94      0.97      0.96       212\n",
            "\n",
            "    accuracy                           0.96       426\n",
            "   macro avg       0.96      0.96      0.96       426\n",
            "weighted avg       0.96      0.96      0.96       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[202  12]\n",
            " [  7 205]]\n"
          ]
        }
      ],
      "source": [
        "evaluate_all_ensemble(resnet, vit, \"ResNet + ViT Ensemble\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📗 ResNet Test Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.90      0.96      0.93       214\n",
            "          AI       0.96      0.89      0.92       212\n",
            "\n",
            "    accuracy                           0.92       426\n",
            "   macro avg       0.93      0.92      0.92       426\n",
            "weighted avg       0.93      0.92      0.92       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[206   8]\n",
            " [ 24 188]]\n",
            "Accuracy: 0.9249\n",
            "\n",
            "📘 ViT Test Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.96      0.83      0.89       214\n",
            "          AI       0.85      0.96      0.90       212\n",
            "\n",
            "    accuracy                           0.89       426\n",
            "   macro avg       0.90      0.89      0.89       426\n",
            "weighted avg       0.90      0.89      0.89       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[177  37]\n",
            " [  8 204]]\n",
            "Accuracy: 0.8944\n",
            "\n",
            "🤝 Ensemble Test Report (ResNet + ViT)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.94      0.96       214\n",
            "          AI       0.94      0.97      0.96       212\n",
            "\n",
            "    accuracy                           0.96       426\n",
            "   macro avg       0.96      0.96      0.96       426\n",
            "weighted avg       0.96      0.96      0.96       426\n",
            "\n",
            "Confusion Matrix:\n",
            " [[202  12]\n",
            " [  7 205]]\n",
            "Accuracy: 0.9554\n"
          ]
        }
      ],
      "source": [
        "print(\"📗 ResNet Test Report\")\n",
        "true_r, pred_r = evaluate_model(resnet, test_loader)\n",
        "print(classification_report(true_r, pred_r, target_names=[\"Real\", \"AI\"]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(true_r, pred_r))\n",
        "print(f\"Accuracy: {accuracy_score(true_r, pred_r):.4f}\\n\")\n",
        "\n",
        "print(\"📘 ViT Test Report\")\n",
        "true_v, pred_v = evaluate_model(vit, test_loader)\n",
        "print(classification_report(true_v, pred_v, target_names=[\"Real\", \"AI\"]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(true_v, pred_v))\n",
        "print(f\"Accuracy: {accuracy_score(true_v, pred_v):.4f}\\n\")\n",
        "\n",
        "print(\"🤝 Ensemble Test Report (ResNet + ViT)\")\n",
        "true_e, pred_e = evaluate_ensemble(resnet, vit, test_loader)\n",
        "print(classification_report(true_e, pred_e, target_names=[\"Real\", \"AI\"]))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(true_e, pred_e))\n",
        "print(f\"Accuracy: {accuracy_score(true_e,pred_e):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "📗 ResNet Test Report\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        Real       0.90      0.96      0.93       214\n",
        "          AI       0.96      0.89      0.92       212\n",
        "\n",
        "    accuracy                           0.92       426\n",
        "   macro avg       0.93      0.92      0.92       426\n",
        "weighted avg       0.93      0.92      0.92       426\n",
        "\n",
        "Confusion Matrix:\n",
        " [[206   8]\n",
        " [ 24 188]]\n",
        "Accuracy: 0.9249\n",
        "\n",
        "📘 ViT Test Report\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        Real       0.96      0.83      0.89       214\n",
        "          AI       0.85      0.96      0.90       212\n",
        "\n",
        "    accuracy                           0.89       426\n",
        "   macro avg       0.90      0.89      0.89       426\n",
        "weighted avg       0.90      0.89      0.89       426\n",
        "\n",
        "Confusion Matrix:\n",
        " [[177  37]\n",
        " [  8 204]]\n",
        "Accuracy: 0.8944\n",
        "\n",
        "🤝 Ensemble Test Report (ResNet + ViT)\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "        Real       0.97      0.94      0.96       214\n",
        "          AI       0.94      0.97      0.96       212\n",
        "\n",
        "    accuracy                           0.96       426\n",
        "   macro avg       0.96      0.96      0.96       426\n",
        "weighted avg       0.96      0.96      0.96       426\n",
        "\n",
        "Confusion Matrix:\n",
        " [[202  12]\n",
        " [  7 205]]\n",
        "Accuracy: 0.9554"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
